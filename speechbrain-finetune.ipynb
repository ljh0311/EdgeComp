{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e673ab0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:39.311313Z",
     "iopub.status.busy": "2023-04-03T10:46:39.310831Z",
     "iopub.status.idle": "2023-04-03T10:46:56.174858Z",
     "shell.execute_reply": "2023-04-03T10:46:56.173537Z"
    },
    "papermill": {
     "duration": 16.872676,
     "end_time": "2023-04-03T10:46:56.177525",
     "exception": false,
     "start_time": "2023-04-03T10:46:39.304849",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCloning into 'SpeechAnalytics'...\r\n",
      "remote: Enumerating objects: 64, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (64/64), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (42/42), done.\u001b[K\r\n",
      "remote: Total 64 (delta 20), reused 54 (delta 10), pack-reused 0\u001b[K\r\n",
      "Receiving objects: 100% (64/64), 8.89 KiB | 2.22 MiB/s, done.\r\n",
      "Resolving deltas: 100% (20/20), done.\r\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "!pip install speechbrain -q\n",
    "import speechbrain as sb\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "!git clone https://github.com/Ishant1/SpeechAnalytics.git\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if not os.path.isdir('/kaggle/working/sample_data'):\n",
    "    os.mkdir('/kaggle/working/sample_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c597e593",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.187224Z",
     "iopub.status.busy": "2023-04-03T10:46:56.185666Z",
     "iopub.status.idle": "2023-04-03T10:46:56.191710Z",
     "shell.execute_reply": "2023-04-03T10:46:56.190782Z"
    },
    "papermill": {
     "duration": 0.012738,
     "end_time": "2023-04-03T10:46:56.193851",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.181113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperpyyaml import load_hyperpyyaml\n",
    "\n",
    "hyperparam_file_address = \"/kaggle/input/hyperparam-wav2vec2/train_with_wav2vec2_with_dropout.yml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2c38111",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.201572Z",
     "iopub.status.busy": "2023-04-03T10:46:56.201268Z",
     "iopub.status.idle": "2023-04-03T10:46:56.207512Z",
     "shell.execute_reply": "2023-04-03T10:46:56.206448Z"
    },
    "papermill": {
     "duration": 0.01266,
     "end_time": "2023-04-03T10:46:56.209659",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.196999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "replacement_dict = {'asvp':{'file':\"/kaggle/input/speech-address/asvp_dict.json\",'replace':[('/content/gdrive/MyDrive/dataset/ERC/ASVP','/kaggle/input/asvpesdspeech-nonspeech-emotional-utterances/ASVP-ESD-Update')]},\n",
    "'meld_dev':{'file':\"/kaggle/input/speech-address/meld_dev_dict.json\",'replace':[('/content/gdrive/MyDrive/dataset/ERC/MELD-RAW-MP3','/kaggle/input/meld-dataset/MELD-RAW'),('mp3','mp4')]},\n",
    "'meld_train':{'file':\"/kaggle/input/speech-address/meld_train_dict.json\",'replace':[('/content/gdrive/MyDrive/dataset/ERC/MELD-RAW-MP3','/kaggle/input/meld-dataset/MELD-RAW'),('mp3','mp4')]},\n",
    "'cremad':{'file':\"/kaggle/input/speech-address/cremad_dict.json\",'replace':[('/content/gdrive/MyDrive/dataset/ERC/CREMAD','/kaggle/input/cremad')]},\n",
    "'iemocap':{'file':\"/kaggle/input/speech-address/iemocap_dict.json\",'replace':[('/content/gdrive/MyDrive/dataset/ERC/IEMOCAP','/kaggle/input/iemocapfullrelease/IEMOCAP_full_release')]}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ffa571",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.217896Z",
     "iopub.status.busy": "2023-04-03T10:46:56.217030Z",
     "iopub.status.idle": "2023-04-03T10:46:56.508842Z",
     "shell.execute_reply": "2023-04-03T10:46:56.507760Z"
    },
    "papermill": {
     "duration": 0.298509,
     "end_time": "2023-04-03T10:46:56.511420",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.212911",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i,d in replacement_dict.items():\n",
    "    with open(d['file'],'r') as f:\n",
    "        dict_add = json.load(f)\n",
    "    \n",
    "    for j,e in dict_add.items():\n",
    "        for to_replace, after_replace in d['replace']:\n",
    "            e['wav'] = e['wav'].replace(to_replace,after_replace)\n",
    "        \n",
    "        dict_add[j] = e\n",
    "        \n",
    "    new_dir = '/kaggle/working/speech-address'\n",
    "    if not os.path.isdir(new_dir):\n",
    "        os.mkdir(new_dir)\n",
    "    with open(d['file'].replace('/kaggle/input/speech-address',new_dir),'w') as f:\n",
    "        json.dump(dict_add,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3adbe9ef",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.520632Z",
     "iopub.status.busy": "2023-04-03T10:46:56.519896Z",
     "iopub.status.idle": "2023-04-03T10:46:56.540054Z",
     "shell.execute_reply": "2023-04-03T10:46:56.539071Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.027129,
     "end_time": "2023-04-03T10:46:56.542251",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.515122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"Recipe for training an emotion recognition system from speech data only using IEMOCAP.\n",
    "The system classifies 4 emotions ( anger, happiness, sadness, neutrality) with wav2vec2.\n",
    "To run this recipe, do the following:\n",
    "> python train_with_wav2vec2.py hparams/train_with_wav2vec2.yaml --data_folder /path/to/IEMOCAP_full_release\n",
    "For more wav2vec2/HuBERT results, please see https://arxiv.org/pdf/2111.02735.pdf\n",
    "Authors\n",
    " * Yingzhi WANG 2021\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import speechbrain as sb\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class EmoIdBrain(sb.Brain):\n",
    "\n",
    "    def compute_forward(self, batch, stage):\n",
    "        \"\"\"Computation pipeline based on a encoder + emotion classifier.\n",
    "        \"\"\"\n",
    "        batch = batch.to(self.device)\n",
    "        wavs, lens = batch.sig\n",
    "\n",
    "        outputs = self.modules.wav2vec2(wavs, lens)\n",
    "\n",
    "        # last dim will be used for AdaptativeAVG pool\n",
    "        outputs = self.hparams.avg_pool(outputs, lens)\n",
    "        outputs = outputs.view(outputs.shape[0], -1)\n",
    "        \n",
    "        outputs = self.hparams.dropout(outputs)\n",
    "        \n",
    "        outputs = self.modules.output_mlp(outputs)\n",
    "        outputs = self.hparams.log_softmax(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def compute_objectives(self, predictions, batch, stage):\n",
    "        \"\"\"Computes the loss using speaker-id as label.\n",
    "        \"\"\"\n",
    "        emoid, _ = batch.emo_encoded\n",
    "\n",
    "        \"\"\"to meet the input form of nll loss\"\"\"\n",
    "        emoid = emoid.squeeze(1)\n",
    "        loss = self.hparams.compute_cost(predictions, emoid)\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics.append(batch.id, predictions, emoid)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def fit_batch(self, batch):\n",
    "        \"\"\"Trains the parameters given a single batch in input\"\"\"\n",
    "\n",
    "        predictions = self.compute_forward(batch, sb.Stage.TRAIN)\n",
    "        loss = self.compute_objectives(predictions, batch, sb.Stage.TRAIN)\n",
    "        loss.backward()\n",
    "        if self.check_gradients(loss):\n",
    "            self.wav2vec2_optimizer.step()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self.wav2vec2_optimizer.zero_grad()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        return loss.detach()\n",
    "\n",
    "    def on_stage_start(self, stage, epoch=None):\n",
    "        \"\"\"Gets called at the beginning of each epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, or sb.Stage.TEST.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Set up statistics trackers for this stage\n",
    "        self.loss_metric = sb.utils.metric_stats.MetricStats(\n",
    "            metric=sb.nnet.losses.nll_loss\n",
    "        )\n",
    "\n",
    "        # Set up evaluation-only statistics trackers\n",
    "        if stage != sb.Stage.TRAIN:\n",
    "            self.error_metrics = self.hparams.error_stats()\n",
    "\n",
    "    def on_stage_end(self, stage, stage_loss, epoch=None):\n",
    "        \"\"\"Gets called at the end of an epoch.\n",
    "        Arguments\n",
    "        ---------\n",
    "        stage : sb.Stage\n",
    "            One of sb.Stage.TRAIN, sb.Stage.VALID, sb.Stage.TEST\n",
    "        stage_loss : float\n",
    "            The average loss for all of the data processed in this stage.\n",
    "        epoch : int\n",
    "            The currently-starting epoch. This is passed\n",
    "            `None` during the test stage.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store the train loss until the validation stage.\n",
    "        if stage == sb.Stage.TRAIN:\n",
    "            self.train_loss = stage_loss\n",
    "\n",
    "        # Summarize the statistics from the stage for record-keeping.\n",
    "        else:\n",
    "            stats = {\n",
    "                \"loss\": stage_loss,\n",
    "                \"error_rate\": self.error_metrics.summarize(\"average\"),\n",
    "            }\n",
    "\n",
    "        # At the end of validation...\n",
    "        if stage == sb.Stage.VALID:\n",
    "\n",
    "            old_lr, new_lr = self.hparams.lr_annealing(stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(self.optimizer, new_lr)\n",
    "\n",
    "            (\n",
    "                old_lr_wav2vec2,\n",
    "                new_lr_wav2vec2,\n",
    "            ) = self.hparams.lr_annealing_wav2vec2(stats[\"error_rate\"])\n",
    "            sb.nnet.schedulers.update_learning_rate(\n",
    "                self.wav2vec2_optimizer, new_lr_wav2vec2\n",
    "            )\n",
    "\n",
    "            # The train_logger writes a summary to stdout and to the logfile.\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch\": epoch, \"lr\": old_lr, \"wave2vec_lr\": old_lr_wav2vec2},\n",
    "                train_stats={\"loss\": self.train_loss},\n",
    "                valid_stats=stats,\n",
    "            )\n",
    "\n",
    "            # Save the current checkpoint and delete previous checkpoints,\n",
    "            self.checkpointer.save_and_keep_only(\n",
    "                meta=stats, min_keys=[\"error_rate\"]\n",
    "            )\n",
    "\n",
    "        # We also write statistics about test data to stdout and to logfile.\n",
    "        if stage == sb.Stage.TEST:\n",
    "            self.hparams.train_logger.log_stats(\n",
    "                {\"Epoch loaded\": self.hparams.epoch_counter.current},\n",
    "                test_stats=stats,\n",
    "            )\n",
    "\n",
    "    def init_optimizers(self):\n",
    "        \"Initializes the wav2vec2 optimizer and model optimizer\"\n",
    "        self.wav2vec2_optimizer = self.hparams.wav2vec2_opt_class(\n",
    "            self.modules.wav2vec2.parameters()\n",
    "        )\n",
    "        self.optimizer = self.hparams.opt_class(self.hparams.model.parameters())\n",
    "\n",
    "        if self.checkpointer is not None:\n",
    "            self.checkpointer.add_recoverable(\n",
    "                \"wav2vec2_opt\", self.wav2vec2_optimizer\n",
    "            )\n",
    "            self.checkpointer.add_recoverable(\"optimizer\", self.optimizer)\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        self.wav2vec2_optimizer.zero_grad(set_to_none)\n",
    "        self.optimizer.zero_grad(set_to_none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326faaaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.550266Z",
     "iopub.status.busy": "2023-04-03T10:46:56.549950Z",
     "iopub.status.idle": "2023-04-03T10:46:56.568305Z",
     "shell.execute_reply": "2023-04-03T10:46:56.567355Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025032,
     "end_time": "2023-04-03T10:46:56.570460",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.545428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data_sample(dataset_dicts, save = True, data_root='sample_data'):\n",
    "\n",
    "  full_data = {}\n",
    "\n",
    "  for i, d in dataset_dicts['datasets'].items():\n",
    "\n",
    "    with open(d['json']) as f:\n",
    "      data_dict = json.load(f)\n",
    "    \n",
    "    indexes = data_dict.keys()\n",
    "    indexes_filtered = random.sample(indexes,int(d['ratio']*len(indexes)))\n",
    "\n",
    "    for j in indexes_filtered:\n",
    "      full_data[i+'_'+j] = data_dict[j]\n",
    "  \n",
    "\n",
    "  full_keys = list(full_data.keys())\n",
    "  full_labels = [d['emo'] for i,d in full_data.items()]\n",
    "\n",
    "  train_val_keys, test_keys, train_val_label, test_label = train_test_split(full_keys, full_labels,\n",
    "                                                    stratify=full_labels, \n",
    "                                                    test_size=dataset_dicts['splits']['test'])\n",
    "  \n",
    "  train_val_ratio = 1 - dataset_dicts['splits']['test']\n",
    "  val_ratio = dataset_dicts['splits']['valid']/train_val_ratio\n",
    "\n",
    "  \n",
    "  train_keys, val_keys, train_label, val_label = train_test_split(train_val_keys, train_val_label,\n",
    "                                                    stratify=train_val_label, \n",
    "                                                    test_size=val_ratio)\n",
    "  \n",
    "\n",
    "  train_data = {i:d for i,d in full_data.items() if i in train_keys}\n",
    "  test_data = {i:d for i,d in full_data.items() if i in test_keys}\n",
    "  val_data = {i:d for i,d in full_data.items() if i in val_keys}\n",
    "\n",
    "  if save:\n",
    "    with open(f\"sample_data/train.json\",'w') as f:\n",
    "      json.dump(train_data,f)\n",
    "\n",
    "    with open(f\"sample_data/test.json\",'w') as f:\n",
    "      json.dump(test_data,f)\n",
    "\n",
    "    with open(f\"sample_data/dev.json\",'w') as f:\n",
    "      json.dump(val_data,f)\n",
    "\n",
    "    keys = {'train':list(train_data.keys()),\n",
    "            'test':list(test_data.keys()),\n",
    "            'val':list(val_data.keys()),\n",
    "            }\n",
    "    \n",
    "    with open(f\"sample_data/keys.json\",'w') as f:\n",
    "      json.dump(keys,f)\n",
    "  \n",
    "  else:\n",
    "    return train_data, test_data, val_data\n",
    "\n",
    "\n",
    "def get_data_from_ids(ids, dataset_dicts, filename=None):\n",
    "\n",
    "  all_ids = {}\n",
    "  for i,d in dataset_dicts['datasets'].items():\n",
    "    with open(d['json'],'r') as f:\n",
    "      data_ = json.load(f)\n",
    "    all_ids[i] = data_\n",
    "\n",
    "  final_data = {}\n",
    "  for i in ids:\n",
    "    data = i.split('_')[0]\n",
    "    uid = i.replace(data+'_','')\n",
    "    final_data[i] = all_ids[data][uid]\n",
    "\n",
    "  if filename:\n",
    "    with open(filename,'w') as f:\n",
    "        json.dump(final_data,f)\n",
    "    return filename\n",
    "  else:\n",
    "    return final_data\n",
    "\n",
    "\n",
    "# Define audio pipeline\n",
    "@sb.utils.data_pipeline.takes(\"wav\")\n",
    "@sb.utils.data_pipeline.provides(\"sig\")\n",
    "def audio_pipeline(wav):\n",
    "    \"\"\"Load the signal, and pass it and its length to the corruption class.\n",
    "    This is done on the CPU in the `collate_fn`.\"\"\"\n",
    "    sig = sb.dataio.dataio.read_audio(wav)\n",
    "    return sig\n",
    "\n",
    "# Initialization of the label encoder. The label encoder assignes to each\n",
    "# of the observed label a unique index (e.g, 'spk01': 0, 'spk02': 1, ..)\n",
    "label_encoder = sb.dataio.encoder.CategoricalEncoder()\n",
    "\n",
    "# Define label pipeline:\n",
    "@sb.utils.data_pipeline.takes(\"emo\")\n",
    "@sb.utils.data_pipeline.provides(\"emo\", \"emo_encoded\")\n",
    "def label_pipeline(emo):\n",
    "    yield emo\n",
    "    emo_encoded = label_encoder.encode_label_torch(emo)\n",
    "    yield emo_encoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e37d39f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:56.578269Z",
     "iopub.status.busy": "2023-04-03T10:46:56.577979Z",
     "iopub.status.idle": "2023-04-03T10:46:57.804880Z",
     "shell.execute_reply": "2023-04-03T10:46:57.803760Z"
    },
    "papermill": {
     "duration": 1.233325,
     "end_time": "2023-04-03T10:46:57.807132",
     "exception": false,
     "start_time": "2023-04-03T10:46:56.573807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the sample datasets with a split of {'train': 0.5, 'test': 0.3, 'valid': 0.2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_valid_splits = {'train':0.5,'test':0.3,'valid':0.2}\n",
    "\n",
    "dataset_dicts = {'datasets':{\n",
    "    'cremad':{'json':\"/kaggle/working/speech-address/cremad_dict.json\",'ratio':1},\n",
    "    'iemocap':{'json':\"/kaggle/working/speech-address/iemocap_dict.json\",'ratio':0.5},\n",
    "    'asvp':{'json':\"/kaggle/working/speech-address/asvp_dict.json\",'ratio':1}\n",
    "    },\n",
    " 'splits':train_test_valid_splits\n",
    " }\n",
    "\n",
    "data_root = 'sample_data'\n",
    "get_data_sample(dataset_dicts, save = True,data_root=data_root)\n",
    "\n",
    "string = f\"Created the sample datasets with a split of {train_test_valid_splits}\"\n",
    "os.system(f'echo \\\"{string}\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65aa5498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:57.815742Z",
     "iopub.status.busy": "2023-04-03T10:46:57.815146Z",
     "iopub.status.idle": "2023-04-03T10:46:57.876468Z",
     "shell.execute_reply": "2023-04-03T10:46:57.875340Z"
    },
    "papermill": {
     "duration": 0.068197,
     "end_time": "2023-04-03T10:46:57.878928",
     "exception": false,
     "start_time": "2023-04-03T10:46:57.810731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define datasets. We also connect the dataset with the data processing\n",
    "# functions defined above.\n",
    "datasets = {}\n",
    "data_info = {\n",
    "    \"train\": f\"{data_root}/train.json\",\n",
    "    \"valid\": f\"{data_root}/dev.json\",\n",
    "    \"test\": f\"{data_root}/test.json\",\n",
    "}\n",
    "for dataset in data_info:\n",
    "    datasets[dataset] = sb.dataio.dataset.DynamicItemDataset.from_json(\n",
    "        json_path=data_info[dataset],\n",
    "        replacements={\"data_root\": data_root},\n",
    "        dynamic_items=[audio_pipeline, label_pipeline],\n",
    "        output_keys=[\"id\", \"sig\", \"emo_encoded\"],\n",
    "    )\n",
    "# Load or compute the label encoder (with multi-GPU DDP support)\n",
    "# Please, take a look into the lab_enc_file to see the label to index\n",
    "# mappinng.\n",
    "\n",
    "lab_enc_file = os.path.join('sample_data', \"label_encoder.txt\")\n",
    "label_encoder.load_or_create(\n",
    "    path=lab_enc_file,\n",
    "    from_didatasets=[datasets[\"train\"]],\n",
    "    output_key=\"emo\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b74409c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-03T10:46:57.887521Z",
     "iopub.status.busy": "2023-04-03T10:46:57.887225Z",
     "iopub.status.idle": "2023-04-03T15:50:25.065688Z",
     "shell.execute_reply": "2023-04-03T15:50:25.064129Z"
    },
    "papermill": {
     "duration": 18207.186352,
     "end_time": "2023-04-03T15:50:25.068764",
     "exception": false,
     "start_time": "2023-04-03T10:46:57.882412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strating Training at time: 2023-04-03 10:46:57.940675\n",
      "Stratigng Training for epoch: 3, dropout: 0.1, lr: 0.0001\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e963010dc0ef47bb8eb41bb4c98ebcab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)rocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995446447dd54fbda3bac307ec7fff12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.84k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/transformers/configuration_utils.py:380: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4efaa8e3919d4cfeae823212cb4b110e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/380M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.947]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.82it/s]\n",
      "100%|██████████| 1079/1079 [05:31<00:00,  3.26it/s, train_loss=0.707]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.80it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.25it/s, train_loss=0.554]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.1, lr: 0.0001 at ['CKPT+2023-04-03+11-05-50+00']\n",
      "Stratigng Training for epoch: 3, dropout: 0.1, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:30<00:00,  3.26it/s, train_loss=0.974]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.76it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.24it/s, train_loss=0.735]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.73it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.23it/s, train_loss=0.594]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.1, lr: 0.001 at ['CKPT+2023-04-03+11-24-43+00']\n",
      "Stratigng Training for epoch: 3, dropout: 0.2, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:31<00:00,  3.25it/s, train_loss=0.97]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.78it/s]\n",
      "100%|██████████| 1079/1079 [05:31<00:00,  3.26it/s, train_loss=0.707]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.71it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.24it/s, train_loss=0.55]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.2, lr: 0.0001 at ['CKPT+2023-04-03+11-43-35+00']\n",
      "Stratigng Training for epoch: 3, dropout: 0.2, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:30<00:00,  3.26it/s, train_loss=0.98]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.79it/s]\n",
      "100%|██████████| 1079/1079 [05:30<00:00,  3.26it/s, train_loss=0.735]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.68it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.25it/s, train_loss=0.603]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.2, lr: 0.001 at ['CKPT+2023-04-03+12-02-28+00']\n",
      "Stratigng Training for epoch: 3, dropout: 0.5, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:31<00:00,  3.26it/s, train_loss=1.01]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.74it/s]\n",
      "100%|██████████| 1079/1079 [05:31<00:00,  3.26it/s, train_loss=0.746]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.85it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.25it/s, train_loss=0.572]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.5, lr: 0.0001 at ['CKPT+2023-04-03+12-21-18+00']\n",
      "Stratigng Training for epoch: 3, dropout: 0.5, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.25it/s, train_loss=1.03]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.73it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.24it/s, train_loss=0.788]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.71it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=0.637]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 3, dropout: 0.5, lr: 0.001 at ['CKPT+2023-04-03+12-40-16+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.1, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=0.962]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.54it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.23it/s, train_loss=0.698]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.66it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.22it/s, train_loss=0.542]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.66it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=0.411]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.76it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.20it/s, train_loss=0.29]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.1, lr: 0.0001 at ['CKPT+2023-04-03+13-11-55+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.1, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=0.983]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.67it/s]\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.742]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.60it/s]\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.62]\n",
      "100%|██████████| 432/432 [00:41<00:00, 10.53it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.21it/s, train_loss=0.482]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.63it/s]\n",
      "100%|██████████| 1079/1079 [05:38<00:00,  3.18it/s, train_loss=0.355]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.1, lr: 0.001 at ['CKPT+2023-04-03+13-43-42+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.2, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.965]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.58it/s]\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.722]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.63it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.21it/s, train_loss=0.559]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.60it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.20it/s, train_loss=0.432]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.63it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.20it/s, train_loss=0.31]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.2, lr: 0.0001 at ['CKPT+2023-04-03+14-15-26+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.2, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.24it/s, train_loss=0.976]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.72it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.24it/s, train_loss=0.745]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.75it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=0.59]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.56it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.23it/s, train_loss=0.464]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.66it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.20it/s, train_loss=0.351]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.2, lr: 0.001 at ['CKPT+2023-04-03+14-40-40+00', 'CKPT+2023-04-03+14-47-00+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.5, lr: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.23it/s, train_loss=1.02]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.68it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.22it/s, train_loss=0.765]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.70it/s]\n",
      "100%|██████████| 1079/1079 [05:36<00:00,  3.21it/s, train_loss=0.61]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.54it/s]\n",
      "100%|██████████| 1079/1079 [05:34<00:00,  3.22it/s, train_loss=0.479]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.72it/s]\n",
      "100%|██████████| 1079/1079 [05:37<00:00,  3.20it/s, train_loss=0.36]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.5, lr: 0.0001 at ['CKPT+2023-04-03+15-18-44+00']\n",
      "Stratigng Training for epoch: 5, dropout: 0.5, lr: 0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2Model: ['project_q.bias', 'project_hid.weight', 'quantizer.codevectors', 'quantizer.weight_proj.bias', 'project_hid.bias', 'project_q.weight', 'quantizer.weight_proj.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.24it/s, train_loss=1.04]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.81it/s]\n",
      "100%|██████████| 1079/1079 [05:32<00:00,  3.24it/s, train_loss=0.787]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.69it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.24it/s, train_loss=0.66]\n",
      "100%|██████████| 432/432 [00:39<00:00, 10.82it/s]\n",
      "100%|██████████| 1079/1079 [05:33<00:00,  3.24it/s, train_loss=0.495]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.63it/s]\n",
      "100%|██████████| 1079/1079 [05:35<00:00,  3.22it/s, train_loss=0.387]\n",
      "100%|██████████| 432/432 [00:40<00:00, 10.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training for epoch: 5, dropout: 0.5, lr: 0.001 at ['CKPT+2023-04-03+15-50-13+00', 'CKPT+2023-04-03+15-43-55+00']\n"
     ]
    }
   ],
   "source": [
    "epochs = [3, 5]\n",
    "dropouts = [0.1, 0.2, 0.5]\n",
    "learning_rates = [0.0001, 0.001]\n",
    "# epochs = [1]\n",
    "# dropouts = [0.1]\n",
    "# learning_rates = [0.0001]\n",
    "\n",
    "combinations = list(itertools.product(epochs,dropouts,learning_rates))\n",
    "\n",
    "string = f\"Strating Training at time: {datetime.now()}\"\n",
    "os.system(f'echo \\\"{string}\\\"')\n",
    "\n",
    "for epoch, dropout, lr in combinations:\n",
    "    \n",
    "    string =f\"Stratigng Training for epoch: {epoch}, dropout: {dropout}, lr: {lr}\"\n",
    "    os.system(f'echo \\\"{string}\\\"')\n",
    "    \n",
    "    output_folder = f\"results/{epoch}-{dropout}-{lr}-model\"\n",
    "    os.environ['OUTPUT_DIR'] = output_folder\n",
    "    \n",
    "    overrides = {'data_folder':'/kaggle/input/iemocapfullrelease/IEMOCAP_full_release',\n",
    "     'number_of_epochs':epoch,\n",
    "     'dropout_prob':dropout,\n",
    "     'lr':lr,\n",
    "     'output_folder':output_folder,\n",
    "     'wav2vec2_folder':'wav2vec2_checkpoint'\n",
    "    }\n",
    "    \n",
    "    \n",
    "    with open(hyperparam_file_address) as fin:\n",
    "        hparams = load_hyperpyyaml(fin, overrides)\n",
    "\n",
    "    hparams[\"wav2vec2\"] = hparams[\"wav2vec2\"].to(device=device)\n",
    "    \n",
    "    run_opts = {'device':device}\n",
    "    emo_id_brain = EmoIdBrain(\n",
    "            modules=hparams[\"modules\"],\n",
    "            opt_class=hparams[\"opt_class\"],\n",
    "            hparams=hparams,\n",
    "            run_opts=run_opts,\n",
    "            checkpointer=hparams[\"checkpointer\"],\n",
    "    )\n",
    "    \n",
    "    emo_id_brain.fit(\n",
    "        epoch_counter=emo_id_brain.hparams.epoch_counter,\n",
    "        train_set=datasets[\"train\"],\n",
    "        valid_set=datasets[\"valid\"],\n",
    "        train_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "        valid_loader_kwargs=hparams[\"dataloader_options\"],\n",
    "    )\n",
    "    \n",
    "    save_contents = os.listdir(f\"{output_folder}/save\")\n",
    "    checkpoint_dir = [c for c in save_contents if 'CKPT' in c][0]\n",
    "    \n",
    "    os.environ['CKPT_DIR'] = checkpoint_dir\n",
    "    os.environ['NEW_CKPT_DIR'] = \"ckpt\"\n",
    "    \n",
    "    \n",
    "    !rm $OUTPUT_DIR/save/$CKPT_DIR/wav2vec2_opt.ckpt\n",
    "    !mkdir $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    !cp -r $OUTPUT_DIR/save/$CKPT_DIR/. $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    !rm -r $OUTPUT_DIR/save/$CKPT_DIR\n",
    "    !cp $OUTPUT_DIR/train_log.txt $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    !cp sample_data/keys.json $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    !cp sample_data/test.json $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    !cp sample_data/label_encoder.txt $OUTPUT_DIR/save/$NEW_CKPT_DIR\n",
    "    \n",
    "    string = f\"Finished Training for epoch: {epoch}, dropout: {dropout}, lr: {lr} at {save_contents}\"\n",
    "    os.system(f'echo \\\"{string}\\\"')\n",
    "    \n",
    "\n",
    "!rm -r wav2vec2_checkpoint\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 18244.610001,
   "end_time": "2023-04-03T15:50:34.410142",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-03T10:46:29.800141",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03feecdc6d67471c94d048d302a40c87": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07e9fccd515544cbba2e460e5d41778c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "0ceed9008b73465e9a22c8323141f32a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0f2b1426ae994d04afafceeb866a3037": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "16ba6277b6dc43e383938db0973d574f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3bc23c37d8054810bcf9b4e75c4a14f8",
       "placeholder": "​",
       "style": "IPY_MODEL_03feecdc6d67471c94d048d302a40c87",
       "value": "Downloading (…)rocessor_config.json: 100%"
      }
     },
     "1c5f3c39a1504246971fc8429c53262b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "25eb7d80a2ea42b59b408856faefe85d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "2a5e36e678a64220906de595d036b13c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0ceed9008b73465e9a22c8323141f32a",
       "placeholder": "​",
       "style": "IPY_MODEL_4abbcd3d8e214081b100036fb8b0b084",
       "value": " 159/159 [00:00&lt;00:00, 6.06kB/s]"
      }
     },
     "2fd48038faa14baf88d5dc08640f9424": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "35319185c45146cab0fbc3fee93f0c88": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_fafb813e03884103b797c5caaca0f158",
       "max": 159,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_de7ca8e3ce814c849877be3ecb979036",
       "value": 159
      }
     },
     "3b7ed7f28b874de283cc66013065e712": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7897c64a522c490b99cb63a4a1a0d3d0",
       "placeholder": "​",
       "style": "IPY_MODEL_2fd48038faa14baf88d5dc08640f9424",
       "value": "Downloading pytorch_model.bin: 100%"
      }
     },
     "3bc23c37d8054810bcf9b4e75c4a14f8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "42e58f58c5f6462db14548a44a3c9b56": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1c5f3c39a1504246971fc8429c53262b",
       "max": 1842,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_c5bbe78aa8d74fbabbc14e489aacaaa1",
       "value": 1842
      }
     },
     "4792c92de32f417e8f9ce22b7742fb0f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d1368aa002d3455d92cbf4467d4256da",
       "placeholder": "​",
       "style": "IPY_MODEL_07e9fccd515544cbba2e460e5d41778c",
       "value": " 380M/380M [00:01&lt;00:00, 325MB/s]"
      }
     },
     "4abbcd3d8e214081b100036fb8b0b084": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4efaa8e3919d4cfeae823212cb4b110e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_3b7ed7f28b874de283cc66013065e712",
        "IPY_MODEL_55d0dada5b3045c284e550961f3152ba",
        "IPY_MODEL_4792c92de32f417e8f9ce22b7742fb0f"
       ],
       "layout": "IPY_MODEL_c36512eee0244cb7866235fca5e11e62"
      }
     },
     "55d0dada5b3045c284e550961f3152ba": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_778c253c7f6d426ab27223a98cf9da4d",
       "max": 380267417,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_25eb7d80a2ea42b59b408856faefe85d",
       "value": 380267417
      }
     },
     "731a652b5c934384bb92cedfbc005043": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "778c253c7f6d426ab27223a98cf9da4d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7897c64a522c490b99cb63a4a1a0d3d0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "941782d58c53443e8b6350fbc4586140": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_f4aa810be74242a3aaf6bafc21d73741",
       "placeholder": "​",
       "style": "IPY_MODEL_df5bf0f7abf34374b76a9eed4fa8d5c0",
       "value": " 1.84k/1.84k [00:00&lt;00:00, 75.2kB/s]"
      }
     },
     "995446447dd54fbda3bac307ec7fff12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b68a8957bfcc44288bed7702e28d01d1",
        "IPY_MODEL_42e58f58c5f6462db14548a44a3c9b56",
        "IPY_MODEL_941782d58c53443e8b6350fbc4586140"
       ],
       "layout": "IPY_MODEL_731a652b5c934384bb92cedfbc005043"
      }
     },
     "b68a8957bfcc44288bed7702e28d01d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0f2b1426ae994d04afafceeb866a3037",
       "placeholder": "​",
       "style": "IPY_MODEL_f825c9d2cac3415ea85ff6ded526f25b",
       "value": "Downloading (…)lve/main/config.json: 100%"
      }
     },
     "c36512eee0244cb7866235fca5e11e62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5bbe78aa8d74fbabbc14e489aacaaa1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d1368aa002d3455d92cbf4467d4256da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "de7ca8e3ce814c849877be3ecb979036": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "df5bf0f7abf34374b76a9eed4fa8d5c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "e963010dc0ef47bb8eb41bb4c98ebcab": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_16ba6277b6dc43e383938db0973d574f",
        "IPY_MODEL_35319185c45146cab0fbc3fee93f0c88",
        "IPY_MODEL_2a5e36e678a64220906de595d036b13c"
       ],
       "layout": "IPY_MODEL_fed01463cd824e299263aab6e6ba889f"
      }
     },
     "f4aa810be74242a3aaf6bafc21d73741": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f825c9d2cac3415ea85ff6ded526f25b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "fafb813e03884103b797c5caaca0f158": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fed01463cd824e299263aab6e6ba889f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
